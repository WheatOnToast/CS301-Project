{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1 (Take Home) - 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS 1.A - 20 points\n",
    "\n",
    "In [this video](https://www.youtube.com/watch?v=ho6JXE3EbZ8) the author explains how to extract various visualizations of what CNNs learn. [Your course site](https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/cnn/cnn-example-architectures/visualizing-what-convnets-learn.html) also covers the topic. \n",
    "\n",
    "Using the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), train a ResNet-50 based CNN on the classification task of $K=9$ classes (filter out the class `ship`) and create the following visualizations for first, middle and last blocks of ResNet-50. You are free to select a class to showcase such visualizations.\n",
    "\n",
    "* Visualizing intermediate convnet outputs (“intermediate activations”). This is useful to understand how successive convnet layers transform their input.\n",
    "\n",
    "* Visualizing convnets filters. This is useful to understand precisely what visual pattern or concept each filter in a convnet is receptive to.\n",
    "\n",
    "* Visualizing heatmaps of class activation in an image. This is useful to understand which part of an image where identified as belonging to a given class, and thus allows to localize objects in images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET 50 WITHOUT SHIPS\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "\n",
    "NUM_CLASSES = 9\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "train_mask = np.where(y_train != 8)[0]\n",
    "test_mask = np.where(y_test != 8)[0]\n",
    "\n",
    "X_train = X_train[train_mask] #image RGB values excluding 8 (ships)\n",
    "y_train = y_train[train_mask] #label values excluding 8 (ships)\n",
    "\n",
    "X_test = X_test[test_mask]  #image RGB values excluding 8 (ships)\n",
    "y_test = y_test[test_mask] #label values excluding 8 (ships)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                    optimizer=\"sgd\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "#history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_data=(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Compiled on 10/22/2023\n",
    "\n",
    "Epoch 1/10\n",
    "1407/1407 [==============================] - 437s 308ms/step - loss: 8.1892 - accuracy: 0.1011 - val_loss: 8.0799 - val_accuracy: 0.0883\n",
    "Epoch 2/10\n",
    "1407/1407 [==============================] - 433s 307ms/step - loss: 7.9276 - accuracy: 0.1049 - val_loss: 8.2936 - val_accuracy: 0.0973\n",
    "Epoch 3/10\n",
    "1407/1407 [==============================] - 455s 323ms/step - loss: 7.8094 - accuracy: 0.1060 - val_loss: 8.3251 - val_accuracy: 0.0905\n",
    "Epoch 4/10\n",
    "1407/1407 [==============================] - 511s 363ms/step - loss: 7.7946 - accuracy: 0.1061 - val_loss: 7.7418 - val_accuracy: 0.1069\n",
    "Epoch 5/10\n",
    "1407/1407 [==============================] - 509s 361ms/step - loss: 7.7918 - accuracy: 0.1063 - val_loss: 7.8850 - val_accuracy: 0.0997\n",
    "Epoch 6/10\n",
    "1407/1407 [==============================] - 463s 329ms/step - loss: 7.6843 - accuracy: 0.1065 - val_loss: 7.6634 - val_accuracy: 0.1075\n",
    "Epoch 7/10\n",
    "1407/1407 [==============================] - 442s 314ms/step - loss: 7.6665 - accuracy: 0.1069 - val_loss: 7.6485 - val_accuracy: 0.1075\n",
    "Epoch 8/10\n",
    "1407/1407 [==============================] - 433s 308ms/step - loss: 7.6425 - accuracy: 0.1071 - val_loss: 7.6217 - val_accuracy: 0.1101\n",
    "Epoch 9/10\n",
    "1407/1407 [==============================] - 434s 308ms/step - loss: 7.6235 - accuracy: 0.1071 - val_loss: 7.6292 - val_accuracy: 0.1077\n",
    "Epoch 10/10\n",
    "1407/1407 [==============================] - 433s 308ms/step - loss: 7.6222 - accuracy: 0.1071 - val_loss: 7.6446 - val_accuracy: 0.1066\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING A JPEG FROM CIFAR10\n",
    "from PIL import Image\n",
    "plt.imshow(X_train[4])\n",
    "array = X_train[4] * 255\n",
    "array = array.astype(np.uint8)\n",
    "img = Image.fromarray(array)\n",
    "img.save(\"car.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING COVNETS\n",
    "\n",
    "img_path = 'car.jpeg'\n",
    "\n",
    "# We preprocess the image into a 4D tensor\n",
    "#from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = tf.keras.utils.load_img(img_path, target_size=(32, 32))\n",
    "img_tensor = tf.keras.utils.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Its shape is (1, 150, 150, 3)\n",
    "print(img_tensor.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]][1:]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 1], cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING CONVNET FILTERS\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "layer_name = 'block3_conv1'\n",
    "filter_index = 0\n",
    "\n",
    "layer_output = model.get_layer(layer_name).output\n",
    "loss = tf.keras.backend.mean(layer_output[:, :, :, filter_index])\n",
    "        \n",
    "grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "grads /= (tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(grads))) + 1e-5)\n",
    "\n",
    "iterate = K.function([model.input], [loss, grads])\n",
    "\n",
    "import numpy as np\n",
    "loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])\n",
    "\n",
    "# We start from a gray image with some noise\n",
    "input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n",
    "\n",
    "# Run gradient ascent for 40 steps\n",
    "step = 1.  # this is the magnitude of each gradient update\n",
    "for i in range(40):\n",
    "    # Compute the loss value and gradient value\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    # Here we adjust the input image in the direction that maximizes the loss\n",
    "    input_img_data += grads_value * step\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_pattern(layer_name, filter_index, size=150):\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    \n",
    "    # ADDED ONE OF THE CIFAR10 IMAGES FOR THE VISUALIZATION\n",
    "    input_img_data = np.resize(X_train[4], (1, 32, 32, 3))\n",
    "\n",
    "    step = 1.\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)\n",
    "\n",
    "plt.imshow(generate_pattern('block3_conv1', 0))\n",
    "plt.show()\n",
    "\n",
    "for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']:\n",
    "    size = 32\n",
    "    margin = 5\n",
    "\n",
    "    # This a empty (black) image where we will store our results.\n",
    "    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n",
    "\n",
    "    for i in range(8):  # iterate over the rows of our results grid\n",
    "        for j in range(8):  # iterate over the columns of our results grid\n",
    "            # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n",
    "            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n",
    "\n",
    "            # Put the result in the square `(i, j)` of the results grid\n",
    "            horizontal_start = 0\n",
    "            horizontal_end = 32\n",
    "            vertical_start = 0\n",
    "            vertical_end = 32\n",
    "            results[horizontal_start: 32, vertical_start: 32, :] = filter_img\n",
    "\n",
    "    # Display the results grid\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow((results * 255).astype(np.uint8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS 1.B - 20 points\n",
    "\n",
    "In [this notebook](https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/transfer-learning/transfer_learning_tutorial.html) we showcase _transfer learning_ using a pre-trained CNN model. \n",
    "\n",
    "Perform the fine-tunning and feature extraction methods of transfer learning using the same model as in PS-1A, for the class `ship`. \n",
    "\n",
    "Repeat the visualization of PS-1.A before and after  transfer learning and write a conclusive summary as to the relative value of the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET 50 WITH ONLY SHIPS\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "train_mask = np.where(y_train == 8)[0]\n",
    "test_mask = np.where(y_test == 8)[0]\n",
    "\n",
    "X_train = X_train[train_mask] #image RGB values excluding all classes (except ships)\n",
    "y_train = y_train[train_mask] #label values excluding all classes (except ships)\n",
    "\n",
    "X_test = X_test[test_mask]  #image RGB values excluding all classes (except ships)\n",
    "y_test = y_test[test_mask] #label values excluding all classes (except ships)\n",
    " \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                    optimizer=\"sgd\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "#history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_data=(X_test, y_test), batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING A SHIP JPEG FROM CIFAR10\n",
    "from PIL import Image\n",
    "plt.imshow(X_train[4])\n",
    "array = X_train[4] * 255\n",
    "array = array.astype(np.uint8)\n",
    "img = Image.fromarray(array)\n",
    "img.save(\"ship.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING COVNETS\n",
    "\n",
    "img_path = 'ship.jpeg'\n",
    "\n",
    "# We preprocess the image into a 4D tensor\n",
    "#from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = tf.keras.utils.load_img(img_path, target_size=(32, 32))\n",
    "img_tensor = tf.keras.utils.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Its shape is (1, 150, 150, 3)\n",
    "print(img_tensor.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]][1:]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 1], cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING CONVNET FILTERS \n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "layer_name = 'block3_conv1'\n",
    "filter_index = 0\n",
    "\n",
    "layer_output = model.get_layer(layer_name).output\n",
    "loss = tf.keras.backend.mean(layer_output[:, :, :, filter_index])\n",
    "        \n",
    "grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "grads /= (tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(grads))) + 1e-5)\n",
    "\n",
    "iterate = K.function([model.input], [loss, grads])\n",
    "\n",
    "import numpy as np\n",
    "loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])\n",
    "\n",
    "# We start from a gray image with some noise\n",
    "input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n",
    "\n",
    "# Run gradient ascent for 40 steps\n",
    "step = 1.  # this is the magnitude of each gradient update\n",
    "for i in range(40):\n",
    "    # Compute the loss value and gradient value\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    # Here we adjust the input image in the direction that maximizes the loss\n",
    "    input_img_data += grads_value * step\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_pattern(layer_name, filter_index, size=150):\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    \n",
    "    # ADDED ONE OF THE CIFAR10 IMAGES FOR THE VISUALIZATION\n",
    "    input_img_data = np.resize(X_train[4], (1, 32, 32, 3))\n",
    "\n",
    "    step = 1.\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)\n",
    "\n",
    "plt.imshow(generate_pattern('block3_conv1', 0))\n",
    "plt.show()\n",
    "\n",
    "for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']:\n",
    "    size = 32\n",
    "    margin = 5\n",
    "\n",
    "    # This a empty (black) image where we will store our results.\n",
    "    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n",
    "\n",
    "    for i in range(8):  # iterate over the rows of our results grid\n",
    "        for j in range(8):  # iterate over the columns of our results grid\n",
    "            # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n",
    "            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n",
    "\n",
    "            # Put the result in the square `(i, j)` of the results grid\n",
    "            horizontal_start = 0\n",
    "            horizontal_end = 32\n",
    "            vertical_start = 0\n",
    "            vertical_end = 32\n",
    "            results[horizontal_start: 32, vertical_start: 32, :] = filter_img\n",
    "\n",
    "    # Display the results grid\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow((results * 255).astype(np.uint8))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
